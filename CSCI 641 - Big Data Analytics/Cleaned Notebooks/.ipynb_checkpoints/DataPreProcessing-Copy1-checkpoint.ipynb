{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert altmetric dump to CSV \n",
    "This jupyter notebook contains the code utilized to parse the content of over 100GB of data from the altmetric june 2018 dataset.\n",
    "The purpose of this code was to extract just the necessary information from this big dataset, shrinking the size to around 1.5GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __parse_file__ function takes as input the path of a file, opens it and read it line by line extracting the json related to a single altmetric. The parser analyzes the json file and extrat the current features:\n",
    "- __altmetric_id:__  article unique identifier\n",
    "- __title:__  title of the article\n",
    "- __subjects:__  list of subjects of the article\n",
    "- __scopus date:__  list of article subjects\n",
    "- __pubdate:__  publication date of the article\n",
    "- __abstract:__  atricle abstract\n",
    "- __fb_wall_count:__  number of times the article was shared on facebook\n",
    "- __fb_wall_urls:__  list of public posts shared by public pages\n",
    "\n",
    "Being the dataset not complete, we can noticed that not all the articles have all the features, so we performed checks on each one of them and fill the values with numpy.nan in case the feature is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_file(filepath):\n",
    "    result = []\n",
    "\n",
    "    #open the file\n",
    "    with open(filepath) as f:\n",
    "        #list of tication fields we want\n",
    "        citationFields = ['title','subjects','scopus_subjects','pubdate','abstract']\n",
    "        #for each altmetric in the file\n",
    "        for line in f:\n",
    "            #we load the json data\n",
    "            data = json.loads(line)\n",
    "            #we check if we have posts\n",
    "            if(len(data['posts'])):\n",
    "                if 'facebook' in data['posts']: #if we have facobook posts\n",
    "                    content = {}\n",
    "                    article_urls=[]\n",
    "                    \n",
    "                    for post in data['posts']['facebook']:  #we get the links\n",
    "                        article_urls.append(post['url'])\n",
    "\n",
    "                    content['altmetric_id'] = data['altmetric_id'] #altmetricID\n",
    "\n",
    "\n",
    "                    for feature in citationFields:\n",
    "                        if feature in data['citation']:\n",
    "                            content[feature] = data['citation'][feature]  #title_article\n",
    "                        else:\n",
    "                            content[feature] = np.nan\n",
    "\n",
    "                    if 'publisher_subjects' in data['citation']:\n",
    "                        content['publisher_subjects'] = data['citation']['publisher_subjects'][0]['name']  #publisher subject\n",
    "                    else:\n",
    "                        content['publisher_subjects'] = np.nan\n",
    "                    \n",
    "                    content['fb_wall_count'] = len(data['posts']['facebook']) #number of facebook posts\n",
    "\n",
    "                    content['fb_wall_urls'] = article_urls\n",
    "\n",
    "                    result.append(content)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __create_dataset__ function is used to perrofm a walks over all the files available in the dumped altmetric dataset, and call the function __parse_file__ to extract the values needed. \n",
    "Once the functionparse_file returns, create dataset will write the extracted value in a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dataset(dataset_file,data_folder,extension):\n",
    "    #open the csv file in writing mode\n",
    "    with open(dataset_file, 'w') as fd:\n",
    "        csvwriter = csv.writer(fd)\n",
    "        #write the header to file\n",
    "        header = [\"altmetric_id\", \"title\", 'subjects','scopus_subjects',\"publisher_subjects\",\"abstract\",'pubdate' ,\"fb_wall_count\",\"fb_wall_urls\"]\n",
    "        csvwriter.writerow(header)\n",
    "        \n",
    "        #get the number of directories we need to scan to show a progress pencentage\n",
    "        numOfDIrs = sum(os.path.isdir(data_folder+'/'+i) for i in os.listdir(data_folder))\n",
    "        currDirNum=1\n",
    "        #loop over all the directories\n",
    "        for dirpath, dirnames, files in os.walk(data_folder):\n",
    "            print(\"Analyzing directory {}/{}: {}\".format(currDirNum,numOfDIrs,dirpath))\n",
    "            currDirNum+=1\n",
    "            #get all the .txt files\n",
    "            for name in files:\n",
    "                if extension and name.lower().endswith(extension):\n",
    "\n",
    "                    file = os.path.join(dirpath, name)\n",
    "                    \n",
    "                    #parse each file and then write it in the csv\n",
    "                    for article in parse_file(file):\n",
    "                        current_row=[]\n",
    "                        for column in header:\n",
    "                            if column in article:\n",
    "                                current_row.append(article[column])\n",
    "                            else:\n",
    "                                current_row.append(np.nan)\n",
    "                        csvwriter.writerow(current_row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call used to start the whole parsing process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_dataset('/media/mfattoru/Backup Data/altmetric_Dataset/extracted_dataset/altmetrics_dataset.csv',\n",
    "               '/media/mfattoru/Backup Data/altmetric_Dataset/keys',\n",
    "               '.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch reaction information from Facebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define all the libraries we need to fetch the reactions, also we define the class bcolors that will be used to color the terminal output. This will help to detect if errors happened while running the script in background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import csv\n",
    "import sys\n",
    "from time import sleep\n",
    "from pathlib import Path\n",
    "import os\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import datetime\n",
    "from random import randint\n",
    "\n",
    "startSleep = 14\n",
    "endSleep = 18\n",
    "\n",
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function queryGraphApi is used to perform a query to the facebook graph api.\n",
    "It takes as input a properly built query for the graph api, and executes it, returning the response.\n",
    "Given the limits enforced by the number of queries that can be executed each hour for our api key, the function takes care of performing timed sleeps for each query in a way to don't exceed the limit of queries available.\n",
    "In case for external use of the api, the number it's exceeded, the function will sleep enough time to be able to reduce the workload in the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def queryGraphApi(url):\n",
    "    responseCode = 0\n",
    "    tries = 1\n",
    "    while(responseCode !=200 ):\n",
    "        print(\"Try fetch number {}\".format(tries))\n",
    "        response = requests.get(url)\n",
    "        responseCode = response.status_code\n",
    "        tries+=1\n",
    "        sleepTime=20\n",
    "        callCount=eval(response.headers['X-App-Usage'])['call_count']\n",
    "\n",
    "        if response.status_code == 400:  #the page went private\n",
    "            response=400\n",
    "            break\n",
    "        if response.status_code != 200:\n",
    "            \n",
    "            print( bcolors.FAIL + 'ERROR' + bcolors.ENDC + ': API response: ' + bcolors.FAIL+ '{}'.format(response.status_code) + bcolors.ENDC)\n",
    "            \n",
    "            print( bcolors.WARNING + \"WARNING\" + bcolors.ENDC+\": You used already {}/100 calls percentage!\".format(callCount))\n",
    "            if(callCount > 98):\n",
    "                sleepTime = 2*endSleep*(callCount-95)\n",
    "            else:\n",
    "                sleepTime = randint(2*startSleep,2*endSleep)\n",
    "\n",
    "            print( bcolors.OKBLUE + \"INFO\" + bcolors.ENDC + \": Sleeping {} seconds before retrying...\".format(sleepTime))\n",
    "            sleep(sleepTime)\n",
    "    print( bcolors.OKBLUE + \"INFO\" + bcolors.ENDC + \": You used {}/100 calls percentage!\".format(callCount))\n",
    "    if(callCount > 98):\n",
    "        sleepTime = 2*endSleep*(callCount-95)\n",
    "        print( bcolors.WARNING + \"WARNING\" + bcolors.ENDC + \": Sleeping {} seconds to lower the limit...\".format(sleepTime))\n",
    "    else:\n",
    "        sleepTime = randint(startSleep,endSleep)\n",
    "        print( bcolors.OKBLUE + \"INFO\" + bcolors.ENDC + \": Sleeping {} seconds to keep the limit...\".format(sleepTime))\n",
    "    sleep(sleepTime)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function cut_files takes as input:\n",
    "- __filename:__ location of the input file\n",
    "- __numOfLines:__ number of lines that we want to remove from the csv file\n",
    "- __fd:__ A file descriptor used to be sure that the file has been previously properly closed\n",
    "\n",
    "This function will remove from the input filename the defined amount of lines in numOfLines, and will take care of creatink a backup file of the dataset before perforning any operation.\n",
    "This is called when CTRL + C is pressed during the execution of the main script, so that we can save the progress made un to that point and then restore the script starting from where we were left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cut_files(filename,numOfLines,fd):\n",
    "    print( bcolors.WARNING + '\\nYou pressed Ctrl+C!, I\\'m deleting lines from the csv, be patient' + bcolors.ENDC)\n",
    "    fd.close();\n",
    "    counter=0\n",
    "    with open(filename,'r+') as csvfile:\n",
    "        with open(filename+\".bkp\", 'w') as bkpfile:\n",
    "            with open(filename+\".tmp\", 'w') as tmpfile:\n",
    "                bkpWriter = csv.writer(bkpfile)\n",
    "                csvWriter = csv.writer(tmpfile)\n",
    "                reader = csv.reader(csvfile)\n",
    "                for row in reader:\n",
    "                    bkpWriter.writerow(row)\n",
    "                    if(counter==0 or counter > numOfLines):\n",
    "                        csvWriter.writerow(row)\n",
    "                    counter+=1\n",
    "    os.remove(filename)\n",
    "    os.rename(filename+\".tmp\",filename)\n",
    "    print( bcolors.OKGREEN + \"I'm done, deleted {} rows, enjoy!\".format(numOfLines) + bcolors.ENDC)\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function parsePage takes in input the pageID of a facebook page, and returns the number of likes and followers a page had.\n",
    "For some pages the html requested doesn't properly contain the needed values, might be because facebook refuses to serve too may requests in a certain amount of time, and so the function will throw an exception, that it's properly handled in the caller of this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parsePage(pageId):\n",
    "    pageCode = 0\n",
    "    while pageCode != 200:\n",
    "        pageHtml = urlopen(\"https://www.facebook.com/\"+pageId)\n",
    "        pageCode = pageHtml.getcode()\n",
    "        if pageCode != 200:\n",
    "            print( bcolors.FAIL + \"ERROR\" + bcolors.ENDC + \": Page parsing status: \"+ bcolors.FAIL + \"{}\".format(pageCode))\n",
    "            sleep(randInt(45,70))\n",
    "            continue\n",
    "        else:\n",
    "            print( bcolors.OKBLUE + \"INFO\" + bcolors.ENDC + \": parsed page status: \" + bcolors.OKGREEN + \"{}\".format(pageCode) + bcolors.ENDC)\n",
    "            soup = BeautifulSoup(pageHtml, 'html5lib')\n",
    "            soupSelect = soup.select('#pages_side_column ._4bl9 div')\n",
    "\n",
    "            pagelikes = soup.find(id=\"PagesLikesCountDOMID\").contents[0].contents[0]\n",
    "            pagelikes = int(str(pagelikes).replace(\",\",\"\"))\n",
    "            if(pagelikes == 0):\n",
    "                pagelikes = soupSelect[0].contents[0]\n",
    "                pagelikes = str(pagelikes).replace(\",\",\"\")\n",
    "                pagelikes=re.findall('\\d+',pagelikes)\n",
    "                pagelikes = int(pagelikes[0])\n",
    "\n",
    "            pagefollowers = soupSelect[1].contents[0]\n",
    "            pagefollowers = str(pagefollowers).replace(\",\",\"\")\n",
    "            pagefollowers=re.findall('\\d+',pagefollowers)[0]\n",
    "            pagefollowers=int(pagefollowers)\n",
    "\n",
    "            print( bcolors.OKGREEN + \"OK\" + bcolors.ENDC + \": Found {} likes and {} followers!\".format(pagelikes,pagefollowers))\n",
    "            return pagelikes,pagefollowers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main caller for the reaction fetcher, this will take care of opening an input csv file, properly sticking together a request to the facebook graph api, fetch the data and the number of facebook followers, and then write everything to a dataset in csv format.\n",
    "The profram can be interrupted by pressing CTRL + C (Not sure if it's the same signal sent by stop in a jupyter notebook), and it will grafelully save all the results obtained  up to that moment, and delete the already parsed lines from the input_file dataset.\n",
    "Restarting the process will restore it from where it was left, appending the results to the same dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    output_file = \"/media/mfattoru/Backup Data/altmetric_Dataset/extracted_dataset/output_1_shares.csv\"\n",
    "    access_token = \"\" #private\n",
    "    graphurl = \"https://graph.facebook.com/v2.2/\"\n",
    "    fields = \"?fields=updated_time,message,name,caption,description,shares,reactions.type(LIKE).limit(0).summary(total_count).as(like)%2Creactions.type(LOVE).limit(0).summary(total_count).as(love)%2Creactions.type(WOW).limit(0).summary(total_count).as(wow)%2Creactions.type(HAHA).limit(0).summary(total_count).as(haha)%2Creactions.type(SAD).limit(0).summary(total_count).as(sad)%2Creactions.type(ANGRY).limit(0).summary(total_count).as(angry)\"\n",
    "    parameters = \"&access_token={}\".format(access_token)\n",
    "    parsedLines = 0\n",
    "\n",
    "    input_file = \"/media/mfattoru/Backup Data/altmetric_Dataset/extracted_dataset/output_1.csv\"\n",
    "\n",
    "    openOperation = 'w'\n",
    "\n",
    "    try:\n",
    "        os.stat(output_file)\n",
    "        openOperation = 'a'\n",
    "    except:\n",
    "        openOperation = 'w'\n",
    "\n",
    "    with open(output_file, openOperation) as fd:\n",
    "        csvwriter = csv.writer(fd)\n",
    "\n",
    "        header = [\"altmetric_id\", \"title\", 'subjects',\"abstract\",'pubdate' ,\"fb_wall_count\",'scopus_subjects','publisher_subjects',\"fb_wall_urls\"]\n",
    "        emotions = ['like','love','wow','haha','sad','angry']\n",
    "        if openOperation == 'w':\n",
    "            print( bcolors.OKBLUE + \"INFO\" + bcolors.ENDC + \": File didn't exist before, so writing down the header\")\n",
    "            csvwriter.writerow(header+[\"shares\",\"visibility\"]+[\"total_\" + s for s in emotions])\n",
    "\n",
    "\n",
    "        df =  pd.read_csv(input_file)\n",
    "        try:\n",
    "            for i, row in df.iterrows():\n",
    "                linkEmotionList=[]\n",
    "                current_row=[]\n",
    "                totalEmotions={}\n",
    "                visibility = {}\n",
    "                shares = 0\n",
    "                numOfPages = 0\n",
    "                missingDataPages = 0\n",
    "\n",
    "                for link in eval(row['fb_wall_urls']):\n",
    "                    print( bcolors.OKBLUE + \"INFO\" + bcolors.ENDC + \": Parsing and requesting info about: \"+link )\n",
    "                    dataDict = {'link':link}\n",
    "                    # visibility = {}\n",
    "                    linkData = urlparse(link)\n",
    "                    if(linkData.query!=''):\n",
    "                        story=linkData.query.split(\"&\")[0]\n",
    "                        storyId=story.split(\"=\")[1]\n",
    "\n",
    "                        pageId=linkData.query.split(\"&id=\")[1]\n",
    "                    elif(linkData.path!=''):\n",
    "                        storyId = linkData.path.split(\"/\")[3]\n",
    "                        pageId = linkData.path.split(\"/\")[1]\n",
    "                    node = pageId + \"_\" + storyId\n",
    "                    base_url = graphurl + node + fields + parameters\n",
    "\n",
    "                    fb_data = queryGraphApi(base_url)\n",
    "                    if(fb_data == 400):\n",
    "                        print( bcolors.FAIL + \"ERROR\" + bcolors.ENDC + \": The page went private!!\")\n",
    "                        continue\n",
    "\n",
    "\n",
    "                    if 'shares' in fb_data.json():\n",
    "                        shares += fb_data.json()['shares']['count']\n",
    "\n",
    "                    for emotion in emotions:\n",
    "                        if emotion in fb_data.json():\n",
    "\n",
    "                            emotionValue = fb_data.json()[emotion]['summary']['total_count']\n",
    "                            dataDict[emotion] = emotionValue\n",
    "\n",
    "                            if emotion in totalEmotions:\n",
    "                                totalEmotions[emotion]+=emotionValue\n",
    "                            else:\n",
    "                                totalEmotions[emotion]=emotionValue\n",
    "                        else:\n",
    "                            dataDict[emotion]=np.nan\n",
    "\n",
    "                    try:\n",
    "                        likes,followers = parsePage(pageId)\n",
    "                        numOfPages+=1\n",
    "                        if pageId in visibility:\n",
    "                            visibility[pageId]+=followers\n",
    "                        else:\n",
    "                            visibility[pageId]=followers\n",
    "                    except:\n",
    "                        print( bcolors.FAIL + \"ERROR: Unable to get likes and followers!\" + bcolors.ENDC )\n",
    "                        missingDataPages+=1\n",
    "                        likes = np.nan\n",
    "                        followers = np.nan\n",
    "\n",
    "                    dataDict['page_likes'] = likes\n",
    "                    dataDict['page_followers'] = followers\n",
    "\n",
    "                    if 'message' in fb_data.json():\n",
    "                        dataDict['message'] = fb_data.json()['message']\n",
    "                        dataDict['has_text'] = 1\n",
    "                    else:\n",
    "                        dataDict['message'] = np.nan\n",
    "                        dataDict['has_text'] = 0\n",
    "\n",
    "                    linkEmotionList.append(dataDict)\n",
    "\n",
    "                if(len(linkEmotionList) != 0):\n",
    "                    for column in header[:-1]: #we don't save the url, as we are going to overwrite it with the list of dictionary\n",
    "                        if column in row:\n",
    "                            current_row.append(row[column])\n",
    "                        else:\n",
    "                            current_row.append(np.nan)\n",
    "\n",
    "                    total_visibility = 0\n",
    "                    for key,value in visibility.items():\n",
    "                        total_visibility += value\n",
    "\n",
    "                    if(numOfPages == 0):\n",
    "                        numOfPages=1\n",
    "                    missingFollowers = (total_visibility//numOfPages)*missingDataPages\n",
    "                    if(missingFollowers > 0):\n",
    "                        print( bcolors.WARNING + \"WARNING\" + bcolors.ENDC+\": Adding {} followers as {} pages were missing!\".format(missingFollowers,missingDataPages))\n",
    "                        total_visibility += missingFollowers\n",
    "\n",
    "                    current_row.append(linkEmotionList)\n",
    "                    current_row.append(shares)\n",
    "                    current_row.append(total_visibility)\n",
    "\n",
    "\n",
    "                    for key,value in totalEmotions.items():\n",
    "                         current_row.append(value)\n",
    "                    csvwriter.writerow(current_row)\n",
    "                    parsedLines+=1\n",
    "                    print( bcolors.OKGREEN + \"[ {} ] parsed line {}\".format(datetime.datetime.now(),parsedLines) + bcolors.ENDC)\n",
    "                else:\n",
    "                    print( bcolors.WARNING + \"[ {} ] skipped line {}\".format(datetime.datetime.now(),parsedLines) + bcolors.ENDC)\n",
    "                # sleep(18)\n",
    "        except KeyboardInterrupt:\n",
    "            cut_files(input_file,parsedLines,fd)\n",
    "        except Exception as e:\n",
    "            print ( bcolors.FAIL + \"ERROR\" + bcolors.ENDC + \": Failed randomly: \"+str(e))\n",
    "            cut_files(input_file,parsedLines,fd)\n",
    "    print( bcolors.OKGREEN + \"[ {} ] Completed the task\".format(datetime.datetime.now()) + bcolors.ENDC)\n",
    "    cut_files(input_file,parsedLines,fd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restore Dataset Integrity\n",
    "Sometimes can happen that the scraper for the number of followers of a page get's blocked as you've inserted a timeout on each query too small.\n",
    "This piece of code will take care of fixing all the altmertics where the number of followers is zero, or nan, and will refetch the information from faebook. and then save everything in a new dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing necessary libraries, should be already imported if you executed the previous imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import csv\n",
    "import sys\n",
    "from time import sleep\n",
    "from pathlib import Path\n",
    "import os\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import datetime\n",
    "from random import randint\n",
    "from numpy import nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modified version of the parsePage function, which will handle the error of not finding values, and return -1 instead of raising an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parsePage(pageId):\n",
    "    pageCode = 0\n",
    "    tries = 1\n",
    "    while pageCode != 200:\n",
    "        try:\n",
    "            pageHtml = urlopen(\"https://www.facebook.com/\"+pageId)\n",
    "            pageCode = pageHtml.getcode()\n",
    "            print(\"INFO: TRY {} - parsed page status: {}\".format(tries,pageCode))\n",
    "            if pageCode != 200:\n",
    "                print(\"ERROR: Page parsing status {}\".format(pageCode))\n",
    "                sleep(randInt(45,70))\n",
    "                continue\n",
    "            else:\n",
    "                soup = BeautifulSoup(pageHtml, 'html5lib')\n",
    "                soupSelect = soup.select('#pages_side_column ._4bl9 div')\n",
    "                try:\n",
    "                    # temp = soup.find(id=\"PagesLikesCountDOMID\").contents[0].contents[0]\n",
    "                    pagelikes = soup.find(id=\"PagesLikesCountDOMID\").contents[0].contents[0]\n",
    "                    pagelikes = int(str(pagelikes).replace(\",\",\"\"))\n",
    "                    if(pagelikes == 0):\n",
    "                        pagelikes = soupSelect[0].contents[0]\n",
    "                        pagelikes = str(pagelikes).replace(\",\",\"\")\n",
    "                        pagelikes=re.findall('\\d+',pagelikes)\n",
    "                        pagelikes = int(pagelikes[0])\n",
    "\n",
    "                    pagefollowers = soupSelect[1].contents[0]\n",
    "                    pagefollowers = str(pagefollowers).replace(\",\",\"\")\n",
    "                    pagefollowers=re.findall('\\d+',pagefollowers)[0]\n",
    "                    pagefollowers=int(pagefollowers)\n",
    "                except: #crash if can't find the values\n",
    "                    pagelikes=-1\n",
    "                    pagefollowers=-1\n",
    "\n",
    "                print(\"Found {} likes and {} followers!\".format(pagelikes,pagefollowers))\n",
    "                return pagelikes,pagefollowers\n",
    "        except:\n",
    "            if tries > 5:\n",
    "                return -1,-1\n",
    "            else:\n",
    "                pageCode = 404\n",
    "                tries+=1\n",
    "                sleepTime=randint(25,36)\n",
    "                print(\"ERROR: Sleeping {} before trying again :X\".format(sleepTime))\n",
    "                sleep(sleepTime)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fix_visibility function is used to fix each row in a dataframe. it will check if the visibility row, the one that contais the number of followers in total for each page that shared the research article is equal to zero or nan which is clearly an error, and will requery facebook for the necessary information.\n",
    "It might happen that one page between all of the ones who shared the paper is not available maybe because it went private, so we count the number of pages which we are unable to get information, and we replace it's value with the mean value of all the pages we were able to get data from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fix_visibility(row):\n",
    "    if row['visibility'] == 'nan' or row['visibility'] == 0:\n",
    "        print(\"MATCH ID {} - VISIBILITY: {}\".format(row['altmetric_id'],row['visibility']))\n",
    "        visibility = 0\n",
    "        missingDataPages = 0\n",
    "        numOfPages = 0\n",
    "#         likes=0\n",
    "        for struct in eval(row['fb_wall_urls']):\n",
    "            followers=0\n",
    "            \n",
    "            link = struct['link']\n",
    "            print(\"Parsing and requesting info about: \"+link)\n",
    "            \n",
    "            linkData = urlparse(link)\n",
    "            if(linkData.query!=''):\n",
    "                \n",
    "                pageId=linkData.query.split(\"&id=\")[1]\n",
    "                \n",
    "            elif(linkData.path!=''):\n",
    "\n",
    "                pageId = linkData.path.split(\"/\")[1]\n",
    "            \n",
    "            _,followers = parsePage(pageId) \n",
    "            if(followers == -1): #unable to parse the page\n",
    "                missingDataPages+=1\n",
    "            else:\n",
    "                numOfPages+=1\n",
    "                visibility += followers\n",
    "            \n",
    "            sleepTime=randint(25,36)\n",
    "            print(\"Sleeping {} to don't get blocked :(\".format(sleepTime))\n",
    "            sleep(sleepTime)\n",
    "            \n",
    "        #if we can't get info about a page we replace it's number of followere with the mean of the ones we got\n",
    "        if(numOfPages == 0):\n",
    "            numOfPages=1\n",
    "        missingFollowers = (visibility//numOfPages)*missingDataPages\n",
    "        visibility += missingFollowers\n",
    "        print(\"TOTAL VISIBILITY: {}\".format(visibility))\n",
    "        return visibility\n",
    "    else:\n",
    "        return row['visibility']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    output_file = \"/media/mfattoru/Backup Data/altmetric_Dataset/extracted_dataset/altmetrics_dataset_facebook_fixed.csv\"\n",
    "\n",
    "    input_file = \"/media/mfattoru/Backup Data/altmetric_Dataset/extracted_dataset/altmetrics_dataset_facebook.csv\"\n",
    "    \n",
    "    df = pd.read_csv(input_file)\n",
    "    df['visibility'] = df.apply(fix_visibility,axis=1)\n",
    "    df.to_csv(output_file,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restore wrong count of shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import csv\n",
    "import sys\n",
    "from time import sleep\n",
    "from pathlib import Path\n",
    "import os\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import datetime\n",
    "from random import randint\n",
    "from numpy import nan\n",
    "import os\n",
    "HOUR = 900\n",
    "startSleep = 14\n",
    "endSleep = 18\n",
    "\n",
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def queryGraphApi(url):\n",
    "    responseCode = 0\n",
    "    tries = 1\n",
    "    while(responseCode !=200 ):\n",
    "        print(\"Try fetch number {}\".format(tries))\n",
    "        response = requests.get(url)\n",
    "        responseCode = response.status_code\n",
    "        tries+=1\n",
    "        sleepTime=20\n",
    "        callCount=eval(response.headers['X-App-Usage'])['call_count']\n",
    "\n",
    "        if response.status_code == 400:  #the page went private\n",
    "            response=400\n",
    "            break\n",
    "            #or should we return directly withouth wait? depends if the error results in a query or not\n",
    "        if response.status_code != 200:\n",
    "\n",
    "            print( bcolors.FAIL + 'ERROR' + bcolors.ENDC + ': API response: ' + bcolors.FAIL+ '{}'.format(response.status_code) + bcolors.ENDC)\n",
    "\n",
    "            print( bcolors.WARNING + \"WARNING\" + bcolors.ENDC+\": You used already {}/100 calls percentage!\".format(callCount))\n",
    "            if(callCount > 98):\n",
    "                sleepTime = 2*endSleep*(callCount-95)\n",
    "            else:\n",
    "                sleepTime = randint(2*startSleep,2*endSleep)\n",
    "\n",
    "            print( bcolors.OKBLUE + \"INFO\" + bcolors.ENDC + \": Sleeping {} seconds before retrying...\".format(sleepTime))\n",
    "            sleep(sleepTime)\n",
    "    print( bcolors.OKBLUE + \"INFO\" + bcolors.ENDC + \": You used {}/100 calls percentage!\".format(callCount))\n",
    "    if(callCount > 98):\n",
    "        sleepTime = 2*endSleep*(callCount-95)\n",
    "        print( bcolors.WARNING + \"WARNING\" + bcolors.ENDC + \": Sleeping {} seconds to lower the limit...\".format(sleepTime))\n",
    "    else:\n",
    "        sleepTime = randint(startSleep,endSleep)\n",
    "        print( bcolors.OKBLUE + \"INFO\" + bcolors.ENDC + \": Sleeping {} seconds to keep the limit...\".format(sleepTime))\n",
    "    sleep(sleepTime)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cut_files(filename,numOfLines,fd):\n",
    "    print( bcolors.WARNING + '\\nYou pressed Ctrl+C!, I\\'m deleting lines from the csv, be patient' + bcolors.ENDC)\n",
    "    fd.close();\n",
    "    counter=0\n",
    "    with open(filename,'r+') as csvfile:\n",
    "        with open(filename+\".bkp\", 'w') as bkpfile:\n",
    "            with open(filename+\".tmp\", 'w') as tmpfile:\n",
    "                bkpWriter = csv.writer(bkpfile)\n",
    "                csvWriter = csv.writer(tmpfile)\n",
    "                reader = csv.reader(csvfile)\n",
    "                for row in reader:\n",
    "                    bkpWriter.writerow(row)\n",
    "                    if(counter==0 or counter > numOfLines):\n",
    "                        csvWriter.writerow(row)\n",
    "                    counter+=1\n",
    "    os.remove(filename)\n",
    "    os.rename(filename+\".tmp\",filename)\n",
    "    print( bcolors.OKGREEN + \"I'm done, deleted {} rows, enjoy!\".format(numOfLines) + bcolors.ENDC)\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFollowers(row):\n",
    "    if 'page_likes' in row and 'page_followers' in row:\n",
    "\n",
    "        if type(row['page_followers']) is int:  #if it's an int, if not it's an nan\n",
    "            if row['page_followers'] == 0 :\n",
    "                raise Exception('Empty Visibility')\n",
    "            else:\n",
    "                print( bcolors.OKGREEN + \"OK\" + bcolors.ENDC + \": Found {} likes and {} followers!\".format(row['page_likes'],row['page_followers']))\n",
    "                return row['page_likes'],row['page_followers']\n",
    "        else:\n",
    "            return row['page_likes'],row['page_followers']\n",
    "    else:\n",
    "        return np.nan,np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    input_file = \"/media/mfattoru/Backup Data/altmetric_Dataset/extracted_dataset/Split Dataset/Cleaned/cleaned_2.csv\"\n",
    "    output_file = \"/media/mfattoru/Backup Data/altmetric_Dataset/extracted_dataset/Split Dataset/Cleaned/clean_fix_share/cleaned_2_shares.csv\"\n",
    "    access_token = \"\" #private\n",
    "    graphurl = \"https://graph.facebook.com/v2.2/\"\n",
    "    fields = \"?fields=updated_time,message,name,caption,description,shares,reactions.type(LIKE).limit(0).summary(total_count).as(like)%2Creactions.type(LOVE).limit(0).summary(total_count).as(love)%2Creactions.type(WOW).limit(0).summary(total_count).as(wow)%2Creactions.type(HAHA).limit(0).summary(total_count).as(haha)%2Creactions.type(SAD).limit(0).summary(total_count).as(sad)%2Creactions.type(ANGRY).limit(0).summary(total_count).as(angry)\"\n",
    "    parameters = \"&access_token={}\".format(access_token)\n",
    "    parsedLines = 0\n",
    "    lastEqualRows = 0\n",
    "\n",
    "    openOperation = 'w'\n",
    "\n",
    "    try:\n",
    "        os.stat(output_file)\n",
    "        openOperation = 'a'\n",
    "    except:\n",
    "        openOperation = 'w'\n",
    "\n",
    "    with open(output_file, openOperation) as fd:\n",
    "        csvwriter = csv.writer(fd)\n",
    "\n",
    "        header = [\"altmetric_id\", \"title\", 'subjects',\"abstract\",'pubdate' ,\"fb_wall_count\",'scopus_subjects','publisher_subjects',\"fb_wall_urls\"]\n",
    "        emotions = ['like','love','wow','haha','sad','angry']\n",
    "        if openOperation == 'w':\n",
    "            print( bcolors.OKBLUE + \"INFO\" + bcolors.ENDC + \": File didn't exist before, so writing down the header\")\n",
    "            csvwriter.writerow(header+[\"shares\",\"visibility\"]+[\"total_\" + s for s in emotions])\n",
    "\n",
    "        df =  pd.read_csv(input_file)\n",
    "        row_count = len(df.index)\n",
    "\n",
    "        try:\n",
    "            for i, row in df.iterrows():\n",
    "                linkEmotionList=[]\n",
    "                current_row=[]\n",
    "                totalEmotions={}\n",
    "                visibility = {}\n",
    "                shares = 0\n",
    "                numOfPages = 0\n",
    "                missingDataPages = 0\n",
    "                #this line is to recheck the number of shares\n",
    "                if 'shares' in row:\n",
    "                    old_shares = row['shares']\n",
    "                else:\n",
    "                    old_shares = -1\n",
    "\n",
    "                numOfLinks = len(eval(row['fb_wall_urls']))\n",
    "                if numOfLinks <= 1:\n",
    "                    parsedLines += 1\n",
    "                    print( bcolors.OKGREEN + \"GOOD\" + bcolors.ENDC+\": Found only {} link, so the data is correct\".format(numOfLinks))\n",
    "                    print( bcolors.OKGREEN + \"[ {} ] Copied line {}/{}\".format(datetime.datetime.now(),parsedLines,row_count) + bcolors.ENDC)\n",
    "                    csvwriter.writerow(row)\n",
    "                else:\n",
    "                    for link in eval(row['fb_wall_urls']):\n",
    "\n",
    "                        if type(link) is not str:  #it's a dictionary\n",
    "                            linkDict = link\n",
    "                            #hack as i'm lazy\n",
    "                            link = link['link']\n",
    "                        else:  #it's a string\n",
    "                            linkDict = eval(link)\n",
    "\n",
    "                        print( bcolors.OKBLUE + \"INFO\" + bcolors.ENDC + \": Parsing and requesting info about: \"+link )\n",
    "                        dataDict = {'link':link}\n",
    "\n",
    "                        linkData = urlparse(link)\n",
    "                        if(linkData.query!=''):\n",
    "                            story=linkData.query.split(\"&\")[0]\n",
    "                            storyId=story.split(\"=\")[1]\n",
    "\n",
    "                            pageId=linkData.query.split(\"&id=\")[1]\n",
    "                        elif(linkData.path!=''):\n",
    "                            storyId = linkData.path.split(\"/\")[3]\n",
    "                            pageId = linkData.path.split(\"/\")[1]\n",
    "                        node = pageId + \"_\" + storyId\n",
    "                        base_url = graphurl + node + fields + parameters\n",
    "\n",
    "                        fb_data = queryGraphApi(base_url)\n",
    "                        if(fb_data == 400):\n",
    "                            print( bcolors.FAIL + \"ERROR\" + bcolors.ENDC + \": The page went private!!\")\n",
    "                            continue\n",
    "\n",
    "\n",
    "                        if 'shares' in fb_data.json():\n",
    "                            shares += fb_data.json()['shares']['count']\n",
    "\n",
    "                        for emotion in emotions:\n",
    "                            if emotion in fb_data.json():\n",
    "\n",
    "                                emotionValue = fb_data.json()[emotion]['summary']['total_count']\n",
    "                                dataDict[emotion] = emotionValue\n",
    "\n",
    "                                if emotion in totalEmotions:\n",
    "                                    totalEmotions[emotion]+=emotionValue\n",
    "                                else:\n",
    "                                    totalEmotions[emotion]=emotionValue\n",
    "                            else:\n",
    "                                dataDict[emotion]=np.nan\n",
    "\n",
    "                        try:\n",
    "                            likes,followers = getFollowers(linkDict)\n",
    "                            # likes,followers = parsePage(pageId)\n",
    "                            numOfPages+=1\n",
    "                            if pageId in visibility:\n",
    "                                visibility[pageId]+=followers\n",
    "                            else:\n",
    "                                visibility[pageId]=followers\n",
    "                        except:\n",
    "                            print( bcolors.FAIL + \"ERROR: Unable to get likes and followers!\" + bcolors.ENDC )\n",
    "                            missingDataPages+=1\n",
    "                            likes = np.nan\n",
    "                            followers = np.nan\n",
    "\n",
    "                        dataDict['page_likes'] = likes\n",
    "                        dataDict['page_followers'] = followers\n",
    "\n",
    "                        if 'message' in fb_data.json():\n",
    "                            dataDict['message'] = fb_data.json()['message']\n",
    "                            dataDict['has_text'] = 1\n",
    "                        else:\n",
    "                            dataDict['message'] = np.nan\n",
    "                            dataDict['has_text'] = 0\n",
    "\n",
    "                        linkEmotionList.append(dataDict)\n",
    "\n",
    "                    if(len(linkEmotionList) != 0):\n",
    "                        for column in header[:-1]: #we don't save the url, as we are going to overwrite it with the list of dictionary\n",
    "                            if column in row:\n",
    "                                current_row.append(row[column])\n",
    "                            else:\n",
    "                                current_row.append(np.nan)\n",
    "\n",
    "                        total_visibility = 0\n",
    "                        for key,value in visibility.items():\n",
    "                            total_visibility += value\n",
    "\n",
    "                        if(numOfPages == 0):\n",
    "                            numOfPages=1\n",
    "                        missingFollowers = (total_visibility//numOfPages)*missingDataPages\n",
    "                        if(missingFollowers > 0):\n",
    "                            print( bcolors.WARNING + \"WARNING\" + bcolors.ENDC+\": Adding {} followers as {} pages were missing!\".format(missingFollowers,missingDataPages))\n",
    "                            total_visibility += missingFollowers\n",
    "\n",
    "                        current_row.append(linkEmotionList)\n",
    "                        current_row.append(shares)\n",
    "                        current_row.append(total_visibility)\n",
    "                        \n",
    "                        if shares != old_shares:\n",
    "                            lastEqualRows = 0\n",
    "                            print(bcolors.WARNING + \"WARNING\" + bcolors.ENDC+\": Old shares were {}, new shares are: {}\".format(old_shares,shares))\n",
    "                        else:\n",
    "                            lastEqualRows+=1\n",
    "                            print(bcolors.OKGREEN + \"GOOD\" + bcolors.ENDC+\":Last {} had Same shares {} == {}\".format(lastEqualRows,old_shares,shares))\n",
    "\n",
    "\n",
    "                        for key,value in totalEmotions.items():\n",
    "                             current_row.append(value)\n",
    "                        csvwriter.writerow(current_row)\n",
    "                        parsedLines+=1\n",
    "                        print( bcolors.OKGREEN + \"[ {} ] parsed line {}/{}\".format(datetime.datetime.now(),parsedLines,row_count) + bcolors.ENDC)\n",
    "                    else:\n",
    "                        print( bcolors.WARNING + \"[ {} ] skipped line {}/{}\".format(datetime.datetime.now(),parsedLines,row_count) + bcolors.ENDC)\n",
    "                    # sleep(18)\n",
    "        except KeyboardInterrupt:\n",
    "            cut_files(input_file,parsedLines,fd)\n",
    "        # except HTTPSConnectionPool:\n",
    "            #want we to wait that the connection is restored and continue?\n",
    "        except Exception as e:\n",
    "            print ( bcolors.FAIL + \"ERROR\" + bcolors.ENDC + \": Failed randomly: \"+str(e))\n",
    "            cut_files(input_file,parsedLines,fd)\n",
    "            os.system('spd-say \"your program failed randomly\"')\n",
    "    print( bcolors.OKGREEN + \"[ {} ] Completed the task\".format(datetime.datetime.now()) + bcolors.ENDC)\n",
    "    cut_files(input_file,parsedLines,fd)\n",
    "    os.system('spd-say \"your program completed the task\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Dataset with Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from numpy import nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The function __normalize_document__ uses nltk to clean the text from stop words, links and other non readable characters and returns a string with just the filtered tokens\n",
    "\n",
    "- The function __dict_to_string__ is used to convert a dictionary to it's string representation. We do this so we are able to save it as a string object inside our csv, so reading from the csv will be compatible with all the other code we wrote, where we read the string and we use eval to evaluate it to a dictionaty\n",
    "\n",
    "- The function __normalize_fb_posts__ has the same effect of the normalize document function, but it's used to normalize the text written by the pages who shared the article, as the data in that case is contained in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wpt = nltk.WordPunctTokenizer()\n",
    "# nltk.download()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "english_words = set(nltk.corpus.words.words()) #list of all english words\n",
    "\n",
    "def normalize_document(doc):\n",
    "    if type(doc) is str:\n",
    "#         print('string')\n",
    "        # lower case and remove special characters\\whitespaces\n",
    "        doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "        doc = doc.lower()\n",
    "        doc = doc.strip()\n",
    "        # tokenize document\n",
    "        tokens = wpt.tokenize(doc)\n",
    "        # filter stopwords and non english words out of document\n",
    "        filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "        filtered_tokens = [token for token in tokens if token in english_words]\n",
    "        filtered_tokens = [token]\n",
    "        # re-create document from filtered tokens\n",
    "        doc = ' '.join(filtered_tokens)\n",
    "\n",
    "        return doc\n",
    "    else:\n",
    "        return doc     \n",
    "\n",
    "def dict_to_string(dictionary):\n",
    "    return '{'+', '.join(\"'{!s}': {!r}\".format(key,val) for (key,val) in dictionary.items())+'}'\n",
    "\n",
    "def normalize_fb_posts(postsList):\n",
    "\n",
    "    posts = eval(postsList)\n",
    "\n",
    "    for post in posts:\n",
    "        if type(post['message']) is str:\n",
    "            message = normalize_document(post['message'])\n",
    "            post['message'] = message\n",
    "            \n",
    "    return '[' + ', '.join(dict_to_string(x) for x in posts) + ']'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read from the dataset, and apply the normalization of the texts to the title of the article, the abstract and to the messages written by the pages who shared the post.\n",
    "At the end we save the results in a new csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/media/mfattoru/Backup Data/altmetric_Dataset/extracted_dataset/altmetrics_dataset_facebook.csv')\n",
    "df['title'] = df['title'].apply(lambda x: normalize_document(x))\n",
    "df['abstract'] = df['abstract'].apply(lambda x: normalize_document(x))\n",
    "df['fb_wall_urls'] = df['fb_wall_urls'].apply(lambda x: normalize_fb_posts(x))\n",
    "df.to_csv('/media/mfattoru/Backup Data/altmetric_Dataset/extracted_dataset/cleaned_dataset.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sentiment analysis, we used the library textblob from python.\n",
    "\n",
    "This library offers an already trained PatternAnalyzer model for sentiment analysis.\n",
    "The model will return a float value between -1 and 1, which represents the polarity of the analyzed text.\n",
    "\n",
    "We then normalize the values so that the algorithm will return just the integers 1,0 and -1\n",
    "\n",
    "for the bacebook post aalysis, the resulting sentiment is equal to the sum of the sentiments of all the shares, then normalized in the integer form 1,0,and -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    '''\n",
    "    Utility function to clean the text in a tweet by removing \n",
    "    links and special characters using regex.\n",
    "    '''\n",
    "    if type(text) is str:\n",
    "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", text).split())\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def analize_sentiment(text):\n",
    "    '''\n",
    "    Utility function to classify the polarity of a tweet\n",
    "    using textblob.\n",
    "    '''\n",
    "    if type(text) is str:\n",
    "        analysis = TextBlob(clean_text(text))\n",
    "        if analysis.sentiment.polarity > 0:\n",
    "            return 1\n",
    "        elif analysis.sentiment.polarity == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "    else:\n",
    "        return \n",
    "\n",
    "def fb_posts_analysis(postList):\n",
    "#     print(postList)\n",
    "    posts = eval(postList)\n",
    "    res = 0\n",
    "    hasMessages = False\n",
    "    for post in posts:\n",
    "#         print(post)\n",
    "        if type(post['message']) is str:\n",
    "            hasMessages = True\n",
    "            message = normalize_document(post['message'])\n",
    "            analysis = TextBlob(clean_text(post['message']))\n",
    "            res += analysis.sentiment.polarity\n",
    "    \n",
    "    if hasMessages:\n",
    "        if res > 0:\n",
    "            return 1\n",
    "        elif res == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "    else:\n",
    "        return np.nan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the sentiment analysis to the title, abstract and shared posts, and save the values as new features called __title_sentiment__, __abstract_sentiment__, and __facebook_sentiment__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>altmetric_id</th>\n",
       "      <th>title</th>\n",
       "      <th>subjects</th>\n",
       "      <th>abstract</th>\n",
       "      <th>pubdate</th>\n",
       "      <th>fb_wall_count</th>\n",
       "      <th>scopus_subjects</th>\n",
       "      <th>publisher_subjects</th>\n",
       "      <th>fb_wall_urls</th>\n",
       "      <th>shares</th>\n",
       "      <th>visibility</th>\n",
       "      <th>total_like</th>\n",
       "      <th>total_love</th>\n",
       "      <th>total_wow</th>\n",
       "      <th>total_haha</th>\n",
       "      <th>total_sad</th>\n",
       "      <th>total_angry</th>\n",
       "      <th>title_sentiment</th>\n",
       "      <th>abstract_sentiment</th>\n",
       "      <th>facebook_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16937763.0</td>\n",
       "      <td>risk ischemic stroke transient ischemic attack increased days noncarotid noncardiac surgery</td>\n",
       "      <td>['brain']</td>\n",
       "      <td>risk stroke cardiac carotid surgery well established contrast stroke risk association noncardiac noncarotid surgery time course insufficiently known investigated prevalence recent planned surgery ...</td>\n",
       "      <td>2017-02-28T00:00:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Health Sciences</td>\n",
       "      <td>Clinical Sciences</td>\n",
       "      <td>[{'link': 'https://www.facebook.com/permalink.php?story_fbid=947255595408598&amp;id=133050910162408', 'like': 3, 'love': 0, 'wow': 0, 'haha': 0, 'sad': 0, 'angry': 0, 'page_likes': 2096175, 'page_foll...</td>\n",
       "      <td>0</td>\n",
       "      <td>2096651</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31057208.0</td>\n",
       "      <td>understanding targeting uptake hiv testing among gay bisexual men attending sexual health clinics</td>\n",
       "      <td>['acquiredimmunodeficiencysyndrome', 'behavioralsciences']</td>\n",
       "      <td>assessed trends hiv testing outcomes period clinicbased initiatives introduced increase hiv testing among gay bisexual men gbm attending sexual health clinics shcs new south wales nsw cohort hivne...</td>\n",
       "      <td>2017-12-19T00:00:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Health Sciences</td>\n",
       "      <td>Health Psychology</td>\n",
       "      <td>[{'link': 'https://www.facebook.com/permalink.php?story_fbid=1794357743949490&amp;id=174782242573723', 'like': 3, 'love': 0, 'wow': 0, 'haha': 0, 'sad': 0, 'angry': 0, 'page_likes': 1624, 'page_follow...</td>\n",
       "      <td>0</td>\n",
       "      <td>1662</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27637930.0</td>\n",
       "      <td>still lonely social adjustment youth without social anxiety disorder following cognitive behavioral therapy</td>\n",
       "      <td>['psychiatry']</td>\n",
       "      <td>social experiences integral part normative development youth social functioning difficulties related poor outcomes youth anxiety disorders particularly social anxiety disorder experience difficult...</td>\n",
       "      <td>2017-10-01T00:00:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Health Sciences</td>\n",
       "      <td>Clinical Sciences</td>\n",
       "      <td>[{'link': 'https://www.facebook.com/permalink.php?story_fbid=1662682910468753&amp;id=450103581726698', 'like': 0, 'love': 0, 'wow': 0, 'haha': 0, 'sad': 0, 'angry': 0, 'page_likes': 1717, 'page_follow...</td>\n",
       "      <td>1</td>\n",
       "      <td>1770</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28052326.0</td>\n",
       "      <td>usefulness electrical auditory brainstem responses assess functionality cochlear nerve using intracochlear test electrode</td>\n",
       "      <td>['otolaryngology', 'neurology']</td>\n",
       "      <td>use intracochlear test electrode assess integrity functionality auditory nerve cochlear implant ci recipients compare electrical auditory brainstem responses eabr via test electrode eabr responses...</td>\n",
       "      <td>2017-10-01T00:00:00+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>Health Sciences</td>\n",
       "      <td>Zoology</td>\n",
       "      <td>[{'link': 'https://www.facebook.com/permalink.php?story_fbid=992368990906348&amp;id=376404182502835', 'like': 0, 'love': 0, 'wow': 0, 'haha': 0, 'sad': 0, 'angry': 0, 'page_likes': 72, 'page_followers...</td>\n",
       "      <td>1</td>\n",
       "      <td>142</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27585764.0</td>\n",
       "      <td>impact pediatric obesity acute asthma exacerbation japan</td>\n",
       "      <td>['pediatrics', 'allergyandimmunology']</td>\n",
       "      <td>asthma obesity common health problems children study investigated impact obesity children hospitalized acute asthma exacerbation obtained hospital discharge records inpatients aged years diagnosis...</td>\n",
       "      <td>2017-10-18T00:00:00+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>Health Sciences</td>\n",
       "      <td>Immunology</td>\n",
       "      <td>[{'link': 'https://www.facebook.com/permalink.php?story_fbid=939393522865567&amp;id=143587395779521', 'like': 5, 'love': 0, 'wow': 0, 'haha': 0, 'sad': 0, 'angry': 0, 'page_likes': 100253, 'page_follo...</td>\n",
       "      <td>8</td>\n",
       "      <td>206236</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   altmetric_id  \\\n",
       "0    16937763.0   \n",
       "1    31057208.0   \n",
       "2    27637930.0   \n",
       "3    28052326.0   \n",
       "4    27585764.0   \n",
       "\n",
       "                                                                                                                       title  \\\n",
       "0                                risk ischemic stroke transient ischemic attack increased days noncarotid noncardiac surgery   \n",
       "1                          understanding targeting uptake hiv testing among gay bisexual men attending sexual health clinics   \n",
       "2                still lonely social adjustment youth without social anxiety disorder following cognitive behavioral therapy   \n",
       "3  usefulness electrical auditory brainstem responses assess functionality cochlear nerve using intracochlear test electrode   \n",
       "4                                                                   impact pediatric obesity acute asthma exacerbation japan   \n",
       "\n",
       "                                                     subjects  \\\n",
       "0                                                   ['brain']   \n",
       "1  ['acquiredimmunodeficiencysyndrome', 'behavioralsciences']   \n",
       "2                                              ['psychiatry']   \n",
       "3                             ['otolaryngology', 'neurology']   \n",
       "4                      ['pediatrics', 'allergyandimmunology']   \n",
       "\n",
       "                                                                                                                                                                                                  abstract  \\\n",
       "0  risk stroke cardiac carotid surgery well established contrast stroke risk association noncardiac noncarotid surgery time course insufficiently known investigated prevalence recent planned surgery ...   \n",
       "1  assessed trends hiv testing outcomes period clinicbased initiatives introduced increase hiv testing among gay bisexual men gbm attending sexual health clinics shcs new south wales nsw cohort hivne...   \n",
       "2  social experiences integral part normative development youth social functioning difficulties related poor outcomes youth anxiety disorders particularly social anxiety disorder experience difficult...   \n",
       "3  use intracochlear test electrode assess integrity functionality auditory nerve cochlear implant ci recipients compare electrical auditory brainstem responses eabr via test electrode eabr responses...   \n",
       "4  asthma obesity common health problems children study investigated impact obesity children hospitalized acute asthma exacerbation obtained hospital discharge records inpatients aged years diagnosis...   \n",
       "\n",
       "                     pubdate  fb_wall_count  scopus_subjects  \\\n",
       "0  2017-02-28T00:00:00+00:00              1  Health Sciences   \n",
       "1  2017-12-19T00:00:00+00:00              1  Health Sciences   \n",
       "2  2017-10-01T00:00:00+00:00              1  Health Sciences   \n",
       "3  2017-10-01T00:00:00+00:00              2  Health Sciences   \n",
       "4  2017-10-18T00:00:00+00:00              3  Health Sciences   \n",
       "\n",
       "  publisher_subjects  \\\n",
       "0  Clinical Sciences   \n",
       "1  Health Psychology   \n",
       "2  Clinical Sciences   \n",
       "3            Zoology   \n",
       "4         Immunology   \n",
       "\n",
       "                                                                                                                                                                                              fb_wall_urls  \\\n",
       "0  [{'link': 'https://www.facebook.com/permalink.php?story_fbid=947255595408598&id=133050910162408', 'like': 3, 'love': 0, 'wow': 0, 'haha': 0, 'sad': 0, 'angry': 0, 'page_likes': 2096175, 'page_foll...   \n",
       "1  [{'link': 'https://www.facebook.com/permalink.php?story_fbid=1794357743949490&id=174782242573723', 'like': 3, 'love': 0, 'wow': 0, 'haha': 0, 'sad': 0, 'angry': 0, 'page_likes': 1624, 'page_follow...   \n",
       "2  [{'link': 'https://www.facebook.com/permalink.php?story_fbid=1662682910468753&id=450103581726698', 'like': 0, 'love': 0, 'wow': 0, 'haha': 0, 'sad': 0, 'angry': 0, 'page_likes': 1717, 'page_follow...   \n",
       "3  [{'link': 'https://www.facebook.com/permalink.php?story_fbid=992368990906348&id=376404182502835', 'like': 0, 'love': 0, 'wow': 0, 'haha': 0, 'sad': 0, 'angry': 0, 'page_likes': 72, 'page_followers...   \n",
       "4  [{'link': 'https://www.facebook.com/permalink.php?story_fbid=939393522865567&id=143587395779521', 'like': 5, 'love': 0, 'wow': 0, 'haha': 0, 'sad': 0, 'angry': 0, 'page_likes': 100253, 'page_follo...   \n",
       "\n",
       "   shares  visibility  total_like  total_love  total_wow  total_haha  \\\n",
       "0       0     2096651           3           0          0           0   \n",
       "1       0        1662           3           0          0           0   \n",
       "2       1        1770           0           0          0           0   \n",
       "3       1         142           0           0          0           0   \n",
       "4       8      206236          22           1          0           0   \n",
       "\n",
       "   total_sad  total_angry  title_sentiment  abstract_sentiment  \\\n",
       "0          0            0              0.0                 1.0   \n",
       "1          0            0              1.0                 1.0   \n",
       "2          0            0             -1.0                 1.0   \n",
       "3          0            0              0.0                 1.0   \n",
       "4          0            0              1.0                 1.0   \n",
       "\n",
       "   facebook_sentiment  \n",
       "0                 1.0  \n",
       "1                 1.0  \n",
       "2                -1.0  \n",
       "3                 0.0  \n",
       "4                 1.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/media/mfattoru/Backup Data/altmetric_Dataset/extracted_dataset/cleaned_dataset.csv')\n",
    "df['title_sentiment'] = df['title'].apply(lambda x: analize_sentiment(x))\n",
    "df['abstract_sentiment'] = df['abstract'].apply(lambda x: analize_sentiment(x))\n",
    "df['facebook_sentiment'] = df['fb_wall_urls'].apply(lambda x: fb_posts_analysis(x))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now store the values in a new csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('/media/mfattoru/Backup Data/altmetric_Dataset/extracted_dataset/nlp_dataset.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
